# Separaci칩n de datos
En machine learning, dividir los datos en grupos es crucial:

- Train data: es el conjunto de datos con los que el modelo aprende.

- Validation data: se usa para ajustar los hiperpar치metros y evitar el sobreajuste.

- Test data: se utiliza para evaluar el rendimiento final del modelo, asegurando que no ha visto estos datos antes.

En scikit-learn, la funci칩n train_test_split permite dividir los datos en conjuntos de entrenamiento y prueba. Para dividir tambi칠n en validaci칩n, generalmente se realiza en dos pasos:

```python
from sklearn.model_selection import train_test_split
# Dividir los datos en train y test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)

# Dividir el conjunto temporal en validation y test
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)
```
## Por que necesitamos un validation set
La raz칩n principal es que queremos mantener el test set **completamente aislado** hasta el final para obtener una evaluaci칩n genuina del rendimiento del modelo en datos **"nuevos"**. Si ajustamos los hiperpar치metros usando el test set, corremos el riesgo de sobreajustar el modelo a estos datos tambi칠n, y no podremos medir objetivamente su capacidad de generalizaci칩n.

Imag칤nate que el test set es como un examen final que no puedes ver hasta que hayas terminado de estudiar y practicar. El validation set es m치s como un examen de pr치ctica que puedes usar para mejorar tus t칠cnicas sin que influya en tu evaluaci칩n final.

B치sicamente, usar el validation set y dejar el test set intacto garantiza una evaluaci칩n m치s honesta del modelo. 游


#  Model selection and evaluation
**k-fold Cross Validation**: Es el concepto general de dividir los datos en k particiones y usarlas para validar un modelo. 



![alt text](images/kfold.png)


```python
sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)
```
![alt text](image.png)


### **GridSearchCV y RandomizedSearchCV** con k-fold Cross Validation
Tanto **`GridSearchCV`** como **`RandomizedSearchCV`** usan **k-fold Cross Validation** como parte de su funcionamiento para optimizar los hiperpar치metros del modelo.

- `cross_val_score` es una de las formas de implementarlo. **GridSearchCV** y **RandomizedSearchCV** usan **k-fold Cross Validation** como parte de su proceso de b칰squeda de hiperpar치metros, pero a침aden el componente de optimizaci칩n.

- **`GridSearchCV`**:
  - Realiza una b칰squeda exhaustiva sobre un conjunto de hiperpar치metros.
  - Para cada combinaci칩n de hiperpar치metros, se entrena el modelo varias veces usando **k-fold Cross Validation** (el valor de `k` es determinado por el par치metro `cv`).
  - Despu칠s de evaluar cada combinaci칩n en los distintos folds, selecciona la combinaci칩n de hiperpar치metros que tenga la mejor m칠trica promedio (por ejemplo, precisi칩n, F1, etc.).

- **`RandomizedSearchCV`**:
  - Es similar a `GridSearchCV`, pero en lugar de probar todas las combinaciones de hiperpar치metros, selecciona aleatoriamente un n칰mero limitado de combinaciones.
  - Tambi칠n utiliza **k-fold Cross Validation** para evaluar el rendimiento de cada combinaci칩n de hiperpar치metros.
  
Ambos m칠todos, al definir el par치metro `cv`, controlan cu치ntos folds se utilizar치n en la validaci칩n cruzada.

**Ejemplo de uso de `GridSearchCV` con k-fold Cross Validation:**
```python
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Definir el modelo
tree = DecisionTreeClassifier()

# Definir los par치metros a optimizar
param_grid = {
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

# Configurar GridSearchCV con 5-fold cross-validation
grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy')

# Ajustar el modelo
grid_search.fit(X, y)

# Mejor modelo
print(f"Mejores par치metros: {grid_search.best_params_}")
```

### **cross_val_score** y k-fold Cross Validation

- **`cross_val_score`** es una funci칩n que realiza validaci칩n cruzada directamente. Internamente, implementa **k-fold Cross Validation**.
- Puedes usar esta funci칩n para evaluar el rendimiento de un modelo de manera simple sin hacer optimizaci칩n de hiperpar치metros. Lo que hace es dividir los datos en **k** folds, entrenar y evaluar el modelo en cada fold, y luego devolver las puntuaciones obtenidas para cada partici칩n.

**Ejemplo de `cross_val_score`:**
```python
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

# Definir el modelo
tree = DecisionTreeClassifier()

# Evaluar con k-fold cross-validation
scores = cross_val_score(tree, X, y, cv=5, scoring='accuracy')

# Puntuaciones de cada fold
print(f"Puntuaciones: {scores}")
# Promedio de puntuaciones
print(f"Promedio de precisi칩n: {scores.mean()}")
```

- **`cross_val_score`**: Es una implementaci칩n directa de k-fold Cross Validation. Solo se encarga de evaluar un modelo usando validaci칩n cruzada. No realiza optimizaci칩n de hiperpar치metros.

### Resumen

- **S칤**, tanto `GridSearchCV` como `RandomizedSearchCV` utilizan **k-fold Cross Validation** para evaluar cada combinaci칩n de hiperpar치metros.
- **`cross_val_score`** es una funci칩n que implementa **k-fold Cross Validation** de forma directa para evaluar un modelo sin optimizaci칩n de hiperpar치metros.
- La principal diferencia es que `GridSearchCV` y `RandomizedSearchCV` buscan los mejores hiperpar치metros, mientras que `cross_val_score` solo eval칰a el rendimiento de un modelo con los hiperpar치metros dados.