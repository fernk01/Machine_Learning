# M茅tricas de Clasificaci贸n Binaria

[VER goggle matrix de confusion](https://developers.google.com/machine-learning/crash-course/classification/thresholding?hl=es-419)
Las m茅tricas de evaluaci贸n son medidas utilizadas para evaluar el rendimiento de un modelo de aprendizaje autom谩tico. En el caso de los problemas de clasificaci贸n en el aprendizaje supervisado, estas m茅tricas nos ayudan a determinar qu茅 tan bien nuestro modelo est谩 haciendo predicciones categ贸ricas, como clasificar un correo electr贸nico como spam o no spam. [link](https://www.themachinelearners.com/metricas-de-clasificacion/#Precision)

Metricas para la clasificaci贸n:

- Accuracy (exactitud).
- Precision (precisi贸n)
- Recall (exhaustividad).
- F1 Score (Valor-F ($F_1$))

Para cada una de las m茅tricas:

- *TP (True Positive):* Son los valores que el algoritmo clasifica como positivos y que realmente son positivos.
- TN (True Negative): Son los valores que el algoritmo clasifica como negativos y que realmente son negativos.
- FP (False Positive): Falsos positivos, es decir, valores que el algoritmo clasifica como positivo cuando realmente son negativos.
- FN (False Negative): Falsos negativos, es decir, valores que el algoritmo clasifica como negativo cuando realmente son positivos.

![alt text](./images/confusion_matrix.png)
![alt text](image-1.png)

## Accuracy (Exactitud)

La exactitud es una m茅trica que mide la proporci贸n de predicciones correctas que realiza el modelo. Se calcula como la proporci贸n de predicciones correctas sobre el total de predicciones realizadas.

$$ Accuracy = \frac{TP + TN}{TP + TN + FP + FN} $$

## Precision (Precisi贸n)

La precisi贸n mide la proporci贸n de predicciones positivas que fueron correctas. Se calcula como la proporci贸n de verdaderos positivos sobre la suma de verdaderos positivos y falsos positivos.

$$ Precision = \frac{TP}{TP + FP} $$

## Recall (Exhaustividad o Sensibilidad)

El Recall de una clase expresa cuan bien puede el modelo detectar a esa clase.

La exhaustividad mide la proporci贸n de positivos reales que fueron clasificados correctamente. Se calcula como la proporci贸n de verdaderos positivos sobre la suma de verdaderos positivos y falsos negativos.

Es la capacidad del modelo para detectar correctamente los positivos verdaderos (clase 1 en este caso)

$$ Recall = \frac{TP}{TP + FN} $$

## Especificidad

La especificidad mide la proporci贸n de negativos reales que fueron clasificados correctamente. Se calcula como la proporci贸n de verdaderos negativos sobre la suma de verdaderos negativos y falsos positivos.

Mide la proporci贸n de verdaderos negativos sobre el total de negativos reales:

(TNR - True Negative Rate): Es la capacidad del modelo para identificar correctamente los negativos verdaderos (clase 0). Se calcula as铆:

$$ Especificidad = \frac{TN}{TN + FP} $$

## F1 Score (Valor-F ($F_1$))

El valor-F es la media arm贸nica entre la precisi贸n y la exhaustividad. Se calcula como:

$$ F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall} $$

Con $F_1$ entre $0$ y $1$, donde $1$ es la mejor puntuaci贸n posible.

Tenemos cuatro casos posibles para cada clase:

- **Alta precision y alto recall**: el modelo maneja perfectamente esa clase.
- **Alta precision y bajo recall**: el modelo no detecta la clase muy bien, pero cuando lo hace es altamente confiable.
- Baja precisi贸n y alto recall: La clase detecta bien la clase pero tambi茅n incluye muestras de otras clases.
- Baja precisi贸n y bajo recall: El modelo no logra clasificar la clase correctamente.

Cuando tenemos un dataset con desequilibrio, suele ocurrir que obtenemos un alto valor de precisi贸n en la clase Mayoritaria y un bajo recall en la clase Minoritaria.

## Macro Avg (Promedio Macro)

El promedio macro es el promedio no ponderado de las m茅tricas por clase. Se calcula sumando las m茅tricas de cada clase y dividi茅ndolas por el n煤mero de clases.

$$\text{Macro Avg} = \frac{1}{N} \sum_{i=1}^{N} \text{Metric}_i $$

donde $( N )$ es el n煤mero de clases y $ \text{Metric}_i $  es la m茅trica (precisi贸n, recall, F1-Score) para la clase $i$ .

## Weighted Avg (Promedio Ponderado)

El promedio ponderado es el promedio de las m茅tricas por clase, ponderado por el n煤mero de ocurrencias de cada clase (support).

$$ \text{Weighted Avg} = \frac{1}{\sum_{i=1}^{N} \text{Support}_i} \sum_{i=1}^{N} (\text{Support}_i \cdot \text{Metric}_i) $$

donde  $\text{Support}_i$ es el n煤mero de ocurrencias de la clase $ i $ y $ \text{Metric}_i $ es la m茅trica (precisi贸n, recall, F1-Score) para la clase $ i $.

## Curva ROC (Receiver Operating Characteristic)

La curva ROC es una representaci贸n gr谩fica de la sensibilidad frente a la especificidad para un sistema clasificador binario a medida que se var铆a el umbral de discriminaci贸n. La curva ROC se crea trazando la tasa de verdaderos positivos (TPR) contra la tasa de falsos positivos (FPR) en varios valores de umbral. La AUC (Area Under the Curve) es el 谩rea bajo la curva ROC y se utiliza como m茅trica para comparar diferentes modelos. [link](https://www.youtube.com/watch?v=AcbbkCL0dlo)

## Matriz de Confusi贸n

La matriz de confusi贸n es una tabla que se utiliza para describir el rendimiento de un modelo de clasificaci贸n en un conjunto de datos para los que se conoce la verdadera clase. La matriz de confusi贸n tiene dos dimensiones: una dimensi贸n representa las clases reales y la otra dimensi贸n representa las clases predichas por el modelo.

## Ejemplo

_Valores Reales y Predicciones_

- **Valores Reales**: 
  $$[, , , , , , , , , ]$$
- **Predicciones**: 
  $$[, , , , , , , , , ]$$

_Matriz de Confusi贸n_
|                | Predicci贸n:  | Predicci贸n:  |
|----------------|----------------|---------------|
| **Real: **   | TP <br> 3      | FN <br> 2     |
| **Real: **   | FP <br> 1      | TN <br> 4     |

```mathematica
          Predicted Positive | Predicted Negative
          --------------------------------------
Actual Positive  |      TP      |      FN      |
          --------------------------------------
Actual Negative  |      FP      |      TN      |

```
_Matriz de Confusi贸n (segun python)_

|               | Predicci贸n (0): | Predicci贸n (1):  |
|---------------|-------------------|-------------------|
| Real (0):   | TN <br> 4        | FP <br> 1         |
| Real (1):   | FN <br> 2        | TP <br> 3         |


Interpretaci贸n

- **Verdaderos Positivos (TP)**: 3 (El modelo predijo "" correctamente 3 veces).
- **Verdaderos Negativos (TN)**: 4 (El modelo predijo "" correctamente 4 veces).
- **Falsos Positivos (FP)**: 1 (El modelo predijo "" incorrectamente 1 vez).
- **Falsos Negativos (FN)**: 2 (El modelo predijo "" incorrectamente 2 veces).

M茅tricas:

- **Accuracy**: $$\frac{3 + 4}{3 + 4 + 1 + 2} = 0.7$$
- **Precision**: $$\frac{3}{3 + 1} = 0.75$$
- **Recall**: $$\frac{3}{3 + 2} = 0.6$$
- **F1 Score**: $$2 \cdot \frac{0.75 \cdot 0.6}{0.75 + 0.6} = 0.6667$$

```plaintext
              precision    recall  f1-score   support

     (0)     0.66         0.8       0.72       6  
     (1)     0.75         0.6       0.6667     5  

    accuracy                           0.7       11
   macro avg    0.7         0.7       0.6833     11  
weighted avg    0.7         0.7       0.6833     11   
```

# Ejemplo

Claro, te explico cada uno de los t茅rminos y m茅tricas que aparecen en el reporte de clasificaci贸n y la matriz de confusi贸n:

## Matriz de Confusi贸n

La matriz de confusi贸n muestra el rendimiento del modelo en t茅rminos de verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos.

```plaintext
array([[10773,   438],
       [ 1965,  1299]], dtype=int64)
```

- **10773**: Verdaderos negativos (No predicho como No)
- **438**: Falsos positivos (No predicho como Yes)
- **1965**: Falsos negativos (Yes predicho como No)
- **1299**: Verdaderos positivos (Yes predicho como Yes)

## Reporte de Clasificaci贸n

El reporte de clasificaci贸n proporciona varias m茅tricas para evaluar el rendimiento del modelo.

```plaintext
              precision    recall  f1-score   support

          No       0.85      0.96      0.90     11211
         Yes       0.75      0.40      0.52      3264

    accuracy                           0.83     14475
   macro avg       0.80      0.68      0.71     14475
weighted avg       0.82      0.83      0.81     14475
```

### M茅tricas por Clase

- **Precision (Precisi贸n)**: Proporci贸n de verdaderos positivos sobre el total de predicciones positivas.
  
  - `No`: 0.85
  - `Yes`: 0.75

- **Recall (Sensibilidad o Recall)**: Proporci贸n de verdaderos positivos sobre el total de verdaderos positivos y falsos negativos.
  
  - `No`: 0.96
  - `Yes`: 0.40

- **F1-Score**: Media arm贸nica de la precisi贸n y el recall.
  
  - `No`: 0.90
  - `Yes`: 0.52

- **Support**: N煤mero de ocurrencias reales de cada clase en el conjunto de datos.
  
  - `No`: 11211
  - `Yes`: 3264

### M茅tricas Globales

- **Accuracy (Exactitud)**: Proporci贸n de predicciones correctas sobre el total de predicciones.
  
  - `0.83`

- **Macro Avg (Promedio Macro)**: Promedio no ponderado de las m茅tricas por clase.
  
  - `Precision`: 0.80
  - `Recall`: 0.68
  - `F1-Score`: 0.71

- **Weighted Avg (Promedio Ponderado)**: Promedio ponderado de las m茅tricas por clase, ponderado por el n煤mero de ocurrencias de cada clase.
  
  - `Precision`: 0.82
  - `Recall`: 0.83
  - `F1-Score`: 0.81

Estas m茅tricas te ayudan a entender c贸mo se desempe帽a tu modelo en t茅rminos de precisi贸n, sensibilidad y balance entre ambas, tanto a nivel de cada clase como globalmente.

Entiendo, vamos a desglosar c贸mo se calculan las m茅tricas para las filas `No` y `Yes` en el reporte de clasificaci贸n utilizando la matriz de confusi贸n que proporcionaste.

### Matriz de Confusi贸n

```plaintext
array([[10773,   438],
       [ 1965,  1299]], dtype=int64)
```

### Desglose de la Matriz de Confusi贸n

- **10773**: Verdaderos negativos (No predicho como No)
- **438**: Falsos positivos (No predicho como Yes)
- **1965**: Falsos negativos (Yes predicho como No)
- **1299**: Verdaderos positivos (Yes predicho como Yes)

### C谩lculo de las M茅tricas para `No`

- **Precision (Precisi贸n)**:
  $$ \text{Precision}_{No} = \frac{TP_{No}}{TP_{No} + FP_{No}} = \frac{10773}{10773 + 438} = \frac{10773}{11211} \approx 0.96
  $$

- **Recall (Sensibilidad o Recall)**:
  $$ \text{Recall}_{No} = \frac{TP_{No}}{TP_{No} + FN_{No}} = \frac{10773}{10773 + 1965} = \frac{10773}{12738} \approx 0.85 $$

- **F1-Score**:
  $$ \text{F1-Score}_{No} = 2 \cdot \frac{\text{Precision}_{No} \cdot \text{Recall}_{No}}{\text{Precision}_{No} + \text{Recall}_{No}} = 2 \cdot \frac{0.96 \cdot 0.85}{0.96 + 0.85} \approx 0.90 $$

- **Support**:
  
  $$
  \text{Support}_{No} = 11211
  $$

### C谩lculo de las M茅tricas para `Yes`

- **Precision (Precisi贸n)**:
  
  $$
  \text{Precision}_{Yes} = \frac{TP_{Yes}}{TP_{Yes} + FP_{Yes}} = \frac{1299}{1299 + 1965} = \frac{1299}{3264} \approx 0.40
  $$

- **Recall (Sensibilidad o Recall)**:
  
  $$
  \text{Recall}_{Yes} = \frac{TP_{Yes}}{TP_{Yes} + FN_{Yes}} = \frac{1299}{1299 + 438} = \frac{1299}{1737} \approx 0.75
  $$

- **F1-Score**:
  
  $$
  \text{F1-Score}_{Yes} = 2 \cdot \frac{\text{Precision}_{Yes} \cdot \text{Recall}_{Yes}}{\text{Precision}_{Yes} + \text{Recall}_{Yes}} = 2 \cdot \frac{0.40 \cdot 0.75}{0.40 + 0.75} \approx 0.52
  $$

- **Support**:
  
  $$
  \text{Support}_{Yes} = 3264
  $$

### Resumen

- **No**:
  
  - Precision: 0.96
  - Recall: 0.85
  - F1-Score: 0.90
  - Support: 11211

- **Yes**:
  
  - Precision: 0.40
  - Recall: 0.75
  - F1-Score: 0.52
  - Support: 3264

Estas m茅tricas se calculan utilizando las f贸rmulas mencionadas anteriormente y los valores de la matriz de confusi贸n.

## Ejemplo 2:

```
             precision    recall  f1-score   support

          0       0.81      0.88      0.84     11211
          1       0.42      0.30      0.35      3264

   accuracy                           0.75     14475
  macro avg       0.62      0.59      0.60     14475
weighted avg       0.72      0.75      0.73     14475
```

En el ejemplo que proporcionas, podemos calcular tanto el **recall** como la **especificidad** bas谩ndonos en las m茅tricas proporcionadas.

Recuerda:

- **Recall** es la capacidad de detectar los positivos verdaderos, es decir, la fracci贸n de los positivos correctamente predichos.
- **Especificidad** es la capacidad de detectar los negativos verdaderos, es decir, la fracci贸n de los negativos correctamente predichos.

Dado que no tenemos expl铆citos los valores de TP, TN, FP, y FN en tu tabla, usaremos la informaci贸n de **recall** para la clase 1 y podemos deducir la **especificidad** a partir de los datos de la clase 0.

### Recall de la clase 1 (Positivos - "Llover谩"):

El recall ya est谩 dado directamente en tu tabla para la clase 1:

$
\text{Recall clase 1} = 0.30
$

Esto significa que el modelo detecta correctamente el 30% de los d铆as en los que llover谩 (clase 1). En otras palabras, de todos los d铆as en los que llovi贸, el modelo predijo correctamente el 30%.

### Especificidad (Clase 0 - No llover谩):

Para calcular la **especificidad**, usamos la clase 0 (los negativos), ya que la especificidad mide cu谩ntos d铆as el modelo detect贸 correctamente que **no** iba a llover.

Sabemos que el **recall** de la clase 0 es 0.88. Este dato est谩 en tu tabla bajo el encabezado **recall** para la clase 0.

Para calcular la **especificidad**, en este caso, ser铆a el **recall** de la clase 0, porque la clase 0 es la clase negativa en un problema binario. Por lo tanto, podemos decir:

$
\text{Especificidad} = \text{Recall clase 0} = 0.88
$

## Ejemplo 3: 3x3

Para calcular la precisi贸n (precision) de la clase cero a partir de la matriz de confusi贸n, puedes usar la siguiente f贸rmula:

### F贸rmula de Precisi贸n

$$ \text{Precisi贸n} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$

Donde:
- **TP (True Positive)**: Verdaderos positivos, que son las predicciones correctas de la clase cero.
- **FP (False Positive)**: Falsos positivos, que son las predicciones incorrectas de la clase cero (casos que no son cero pero fueron predichos como cero).

### Identificaci贸n de TP y FP

En la matriz que proporcionaste:


|                        | Predicci贸n 0 | Predicci贸n 1 | Predicci贸n 2 |
|------------------------|---------------------|-------------------|---------------------|
| **Real 0**      |    13    |     1     |     1     | 
| **Real 1**      |    0     |    15     |     3     | 
| **Real 2**      |    4     |     1     |    13     |


- **TP (True Positive) para la clase 0**: Es el valor en la fila de "Real 0" y la columna de "Predicci贸n 0", que es **13**.
- **FP (False Positive) para la clase 0**: Es la suma de todos los valores en la columna de "Predicci贸n 0", excepto el de "Real 0". Es decir:
  - De la fila de "Real 1": **0**
  - De la fila de "Real 2": **4**

Por lo tanto:

$$ \text{FP} = 0 + 4 = 4 $$

### C谩lculo de la Precisi贸n

Ahora puedes calcular la precisi贸n para la clase cero:

$$ \text{Precisi贸n} = \frac{TP}{TP + FP} = \frac{13}{13 + 4} = \frac{13}{17} \approx 0.7647 $$

### Resultado

La precisi贸n para la clase cero es aproximadamente **0.7647** o **76.47%**.
### Resumen:

- **Recall (clase 1 - Llover谩)** = 0.30 (detecta correctamente el 30% de los d铆as con lluvia).
- **Especificidad (clase 0 - No llover谩)** = 0.88 (detecta correctamente el 88% de los d铆as sin lluvia).

Estos valores muestran que el modelo es bueno para detectar los d铆as sin lluvia, pero tiene dificultades para detectar los d铆as en que s铆 llover谩.

# M茅tricas de Regresi贸n

1. Ra铆z del Error Cuadr谩tico Medio (RMSE)
2. Error Cuadr谩tico Medio (MSE)
3. Error Absoluto Medio (MAE)
4. Coeficiente de Determinaci贸n ($R^2$)

Para cada una de las m茅tricas:

- $n$ es el n煤mero de observaciones.
- $y_i$ es el valor real.
- $\hat{y}_i$ es el valor predicho.

## Ra铆z del Error Cuadr谩tico Medio (RMSE)

La Ra铆z del Error Cuadr谩tico Medio (RMSE) es una m茅trica que mide la ra铆z cuadrada de la media de los cuadrados de los errores entre los valores predichos y los valores reales. Cuanto menor sea el RMSE, mejor ser谩 el modelo.

$$ RMSE = \sqrt{\frac{1}{n} \cdot \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$
donde:

## Error Cuadr谩tico Medio (MSE)

El Error Cuadr谩tico Medio (MSE) es una m茅trica que mide el promedio de los cuadrados de los errores entre los valores predichos y los valores reales. Cuanto menor sea el MSE, mejor ser谩 el modelo.

$$ MSE = \frac{1}{n} \cdot \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

## Error Absoluto Medio (MAE)

El Error Absoluto Medio (MAE) es una m茅trica que mide el promedio de los valores absolutos de los errores entre los valores predichos y los valores reales. Cuanto menor sea el MAE, mejor ser谩 el modelo.

$$ MAE = \frac{1}{n} \cdot \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

## Coeficiente de Determinaci贸n ($R^2$)

El Coeficiente de Determinaci贸n ($R^2$) es una m茅trica que indica la proporci贸n de la varianza en la variable dependiente que es predecible a partir de la variable independiente. Cuanto m谩s cercano sea el valor de $R^2$ a 1, mejor ser谩 el modelo.

$$ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} $$

## Cuando usar cada m茅trica

- **RMSE**: Es sensible a los valores at铆picos y penaliza los errores grandes.
- **MSE**: Es sensible a los valores at铆picos y penaliza los errores grandes.
- **MAE**: Es menos sensible a los valores at铆picos y no penaliza tanto los errores grandes.
- **$R^2$**: Mide la proporci贸n de la varianza explicada por el modelo. Cuanto m谩s cercano a 1, mejor es el modelo.

# [Curva Roc](roc_curve.md)
