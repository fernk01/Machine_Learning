# K-Nearest Neighbors (K-NN)
El algoritmo de K-Nearest Neighbors (K-NN) es un algoritmo de aprendizaje supervisado utilizado para clasificaci칩n y regresi칩n.

- Es sensible a conjuntos de datos no balanceados.
- Es muy sensible aoutliers.
- La normalizaci칩n de los datos de entrenamiento puede mejorar dr치sticamente su precisi칩n
- Si se aplica en un comjunto de datos desbalanceado, puede ser que el algoritmo siempre prediga la clase mayoritaria. 

## Explicaci칩n del Algoritmo K-NN

1. **Definir K**: Elegir el n칰mero de vecinos m치s cercanos \( K \).
2. **Calcular distancias**: Para cada punto de prueba, calcular la distancia entre este punto y todos los puntos del conjunto de entrenamiento.
3. **Identificar vecinos**: Seleccionar los \( K \) puntos m치s cercanos del conjunto de entrenamiento.
4. **Clasificaci칩n**: 
    - Para clasificaci칩n, asignar la clase m치s com칰n entre los \( K \) vecinos. 
    - Para regresi칩n, calcular la media de los valores de los \( K \) vecinos.

## C치lculo de la Distancia

Una de las distancias m치s comunes utilizadas en K-NN es la distancia euclidiana. La f칩rmula para calcular la distancia euclidiana entre dos puntos $( \mathbf{x} = (x_1, x_2, ..., x_n) ) $ y $( \mathbf{y} = (y_1, y_2, ..., y_n) ) $ en un espacio n-dimensional es:

$$ \ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} ] $$

## Ejemplo de C치lculo
[YouTube](https://www.youtube.com/watch?v=0p0o5cmgLdE)

Supongamos que tenemos un conjunto de datos con dos caracter칤sticas $( x_1 )$ y $( x_2 )$ y queremos clasificar un nuevo punto $ ( \mathbf{p} = (2, 3) )$. Consideremos que K = 3. Aqu칤 hay un ejemplo paso a paso:

### Conjunto de Datos de Entrenamiento
| Punto | $( x_1 )$ | $( x_2 $) | Clase | Valor |
|-------|----------|----------|-------| ------|
| \( A \) | 1        | 2        | 0     | 2.5   |
| \( B \) | 2        | 4        | 0     | 3.0   |
| \( C \) | 4        | 4        | 1     | 4.5   |
| \( D \) | 5        | 2        | 1     | 5.0   |

### Calcular Distancias Euclidianas

1. **Distancia entre ($\mathbf{p}$) y (A)**:
  $$ d(\mathbf{p}, A) = \sqrt{(2-1)^2 + (3-2)^2} = \sqrt{1 + 1} = \sqrt{2} \approx 1.41 $$

2. **Distancia entre ($\mathbf{p}$) y (B)**:
  $$ d(\mathbf{p}, B) = \sqrt{(2-2)^2 + (3-4)^2} = \sqrt{0 + 1} = 1$$

3. **Distancia entre ($\mathbf{p}$) y (C)**:
  $$ d(\mathbf{p}, C) = \sqrt{(2-4)^2 + (3-4)^2} = \sqrt{4 + 1} = \sqrt{5} \approx 2.24$$

4. **Distancia entre ($\mathbf{p}$) y (D)**:
  $$d(\mathbf{p}, D) = \sqrt{(2-5)^2 + (3-2)^2} = \sqrt{9 + 1} = \sqrt{10} \approx 3.16$$

### Identificar los 3 Vecinos M치s Cercanos
Las distancias ordenadas son:
- ( B ): 1
- ( A ): 1.41
- ( C ): 2.24

### Clasificaci칩n
Los 3 vecinos m치s cercanos son 
- (B) (Clase 0)
- (A) (Clase 0)
- (C) (Clase 1)

La clase m치s com칰n entre los 3 vecinos m치s cercanos es la clase 0. Por lo tanto, el nuevo punto $( \mathbf{p} = (2, 3))$ se clasifica como clase 0.

### Regresi칩n
Para predecir el valor del nuevo punto $(\mathbf{p})$, se toma la media de los valores de los 3 vecinos m치s cercanos:
- ( B ): 3.0
- ( A ): 2.5
- ( C ): 4.5

$$ \text{Valor predicho} = \frac{3.0 + 2.5 + 4.5}{3} = 3.33 $$

## Implementaci칩n en Python

Aqu칤 hay un ejemplo sencillo de implementaci칩n de K-NN en Python usando el c치lculo de la distancia euclidiana:

```python
import numpy as np
from collections import Counter

def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2)**2))

def knn_classify(training_data, training_labels, new_point, k):
    distances = []
    for i in range(len(training_data)):
        distance = euclidean_distance(training_data[i], new_point)
        distances.append((distance, training_labels[i]))
    
    distances.sort(key=lambda x: x[0])
    nearest_neighbors = distances[:k]
    
    classes = [label for _, label in nearest_neighbors]
    majority_class = Counter(classes).most_common(1)[0][0]
    
    return majority_class

# Datos de entrenamiento
training_data = np.array([[1, 2], [2, 4], [4, 4], [5, 2]])
training_labels = np.array([0, 0, 1, 1])

# Punto nuevo
new_point = np.array([2, 3])

# Clasificaci칩n
k = 3
predicted_class = knn_classify(training_data, training_labels, new_point, k)
print(f"La clase predicha para el punto {new_point} es: {predicted_class}")
```

Este c칩digo clasifica el nuevo punto \( [2, 3] \) como clase 0 utilizando el algoritmo K-NN con \( K = 3 \).

## KNN sklearn
- Entrenamiento (fit): En la etapa de "entrenamiento" de K-NN, en realidad no hay un proceso de aprendizaje como en otros algoritmos de aprendizaje supervisado. En lugar de eso, K-NN simplemente almacena el conjunto de datos de entrenamiento completo. Esto incluye tanto las caracter칤sticas (features) como las etiquetas de clase asociadas.
- Predicci칩n (predict): Cuando se solicita una predicci칩n para un nuevo punto de datos, el algoritmo realiza los siguientes pasos:
    - Calcula la distancia entre el nuevo punto y todos los puntos en el conjunto de datos de entrenamiento.
    - Ordena estos puntos seg칰n la distancia calculada.
    - Selecciona los 洧 puntos m치s cercanos.
    - Determina la clase del nuevo punto en funci칩n de la mayor칤a de las clases de los 洧 vecinos m치s cercanos (para un problema de clasificaci칩n) o la media de los valores de los K vecinos (para un problema de regresi칩n).